# 概要: Human-AI協働開発の基本ルール

このドキュメントは、人間とAIエージェントが実際にどのように協働して開発を進めるかの**具体的なワークフロー**を定義します。
`OverviewDesignThinking.md` で定義された設計思想を実現するための**実践的手順**をここにまとめます。

---

## 1. Human-AI協働のためのフロー設計: 考え方

AIエージェントが一貫した品質で作業し、人間とのコミュニケーションが円滑に行われるために、プロセス全体を**「フロー」として標準化**することが不可欠です。

### 1.1 なぜフローを決めるのか？

1. **品質担保**: AIエージェントが常に同じ基準で実装・テストする
2. **コミュニケーション効率**: 人間とのインタラクション（QAループ、確認）が予測可能になる
3. **自己文書化**: 開発プロセス自体がプロジェクトの資産となり、後続の開発者が追跡可能

### 1.2 基本原則: 相互確認（Reciprocal Confirmation）

AIエージェントは、以下のタイミングで**必ず人間の確認を求める**こと:
- **実装前**: 要件が曖昧な場合、推測で進めず不足情報を質問する
- **テスト生成前**: テストシナリオの一覧を提示し、期待値が合っているか確認する
- **実装完了時**: `task check` をパスした上で、安全にコミットしてよいか確認を求める

---

## 2. 開発インターフェース統一: go-task/Taskfile.yml

言語（Python, Go, TypeScript）やフレームワーク（CLI, Web）が増えても、**「AIエージェントへの命令」は統一**します。
我々は `go-task` (Taskfile) を標準インターフェースとして採用します。

### 2.1 標準タスク定義

言語・フレームワークに関わらず、以下のコマンドが動作するように `Taskfile.yml` を維持してください。

| タスク | 目的 | AIへの指示 | コマンド |
|--------|------|-----------|----------|
| **setup** | 依存関係のインストール、環境構築 | 「開発環境を準備して」 | `task setup` |
| **test** | テストの実行 | 「テストを実行して」 | `task test` |
| **lint** | コードスタイルのチェック | 「コードスタイルをチェックして」 | `task lint` |
| **check** | **品質保証の決定版** | 「変更内容を検証して問題ないか確認して」 | `task check` |

### 2.2 品質保証の決定版: `task check`

`task check` は **Lint + Type-Check + Test** を一括実行するAIエージェント作業の**合格基準**です。

#### ルール:
- **コミット前の必須事項**: AIエージェントは、作業完了宣言の前に必ず `task check` をパスさせなければならない
- **タスクチェーン**: 内部で `lint` → `type-check` → `test` が順番に実行される
- **エラー対応**: もし `task check` でエラーが発生した場合、AIエージェントは自動で修正を試みるか、人間に報告して指示を仰ぐ

#### 実践例:
```bash
# 変更を加えた後、必ず実行
$ task check

# 成功例:
✓ lint: passed
✓ type-check: passed
✓ test: 15 passed in 1.2s
✅ check: all quality gates passed

# 失敗例:
✗ lint: 3 issues found (fixable)
→ AIエージェントは自動で `task lint-fix` を実行して修正を試みる

✗ test: 2 failed
→ AIエージェントはテスト失敗の原因を分析し、実装かテストのどちらかを見直す
```

---

## 3. 実装計画とタスク管理: PLAN.md / WorkingLog.md

AIエージェントが自律的に開発を進める際、その過程を人間に透明化し、後続のタスクへのコンテキストを維持することが不可欠です。

### 3.1 PLAN.md: 実装計画・タスク管理

**原則**: 実装やテストの追加・修正を行う際は、事前に `docs/PLAN.md` にチェックリストを作成し、タスクを可視化します。

#### フロー:
1. **ユーザーとの相談・要件定義**: タスクの目的と範囲を明確にする
2. **PLAN.md へのタスク列挙**: 日付または機能名で新しいセクションを作成し、ToDoリストを箇条書きで列挙
3. **実装入力**: 完了した項目を `[x]` でチェックしながら進捗を可視化
4. **タスク完了とコミット**: 全て完了したらコミットし、WorkingLog.md へ記録

#### 記述例（PLAN.md）:
```markdown
## 2026-01-05: コードレビュー機能の追加

- [x] レビュー対象ファイルの探索機能実装
- [x] AIエージェント（ReaderAgent）のCLI統合
- [x] レビュー結果をレポートとして出力
- [ ] テストの実装（ひな形まで）
- [ ] READMEへの使用方法の追記
```

### 3.2 WorkingLog.md: 作業ログ・自己記録

**原則**: まとまった作業（機能実装、リファクタリング、設定変更など）をコミットするタイミングで、必ず `docs/WorkingLog.md` を更新します。

#### 記述ルール:
- **日本語で記述**: 誰が読んでも理解できる平易な日本語
- **逆時系列**: 最新の情報を一番上に追記する（履歴が自然に溜まる）
- **簡潔な箇条書き**: 実施した内容を箇条書きで簡潔にまとめる
- **理由・判断の記録**: 設計上の重要なトレードオフ（SPECの解釈変更など）は、なぜその選択をしたのかを明記

#### 記述例（WorkingLog.md）:
```markdown
## 2026-01-05: コードレビュー機能の実装完了

### 実施内容
- src/agents/reviewer.py: レビューエージェントを新規作成
- tests/test_reviewer.py: 単体テストを実装（4テストケース）
- CLI統合: main.py に `review` コマンドを追加

### 判断・選択
- Agentツールの選択: 当初Crushを検討していたが、Qwenの方がドキュメント構造理解の精度が高かったためQwen採用
- テスト方針: エンドツーエンドテストはAI・人間の確認が複雑になるため、単体テストまでにとどめた

### 参照
- PLAN.md: 2026-01-05 セクション
- SPEC.md: FR-003 コードレビュー機能
- BLUEPRINT.md: Agent統合アーキテクチャ
```

### 3.3 報告の簡素化

AIエージェントは、開発完了時に以下のように報告するだけで十分です:
> 「コードレビュー機能の実装が完了しました。PLAN.md のタスクをすべて完了し、WorkingLog.md に詳細を追記しました。」

重複した詳細説明は不要（WorkingLog.md にすべて記録されているため）。

---

## 4. 実践ワークフロー: QAヒアリング → TDG実行

人間は完璧な仕様書をいきなり書けません。AIエージェントの役割は、単に実装することだけでなく、**「人間から曖昧さを取り除く質問をする」**ことでもあります。

### 4.1 要件定義のヒアリング・ループ

#### 目的:
`SPEC.md` や `BLUEPRINT.md` を**人間との対話を通じて確定**させる

#### ステップ:
1. **Human**: 「こういう機能が欲しい」と自然言語で投げる
2. **AI**: 不足している情報を特定し、質問する
   - **エッジケース**: 「入力が空の場合の挙動は？」
   - **エラー処理**: 「ネットワークエラー時のリトライ戦略は？」
   - **制約条件**: 「APIレート制限がある場合、どのように回避しますか？」
3. **Refinement**: 対話を通じて `SPEC.md` を更新・確定する
4. **Confirm**: 更新した仕様を要約して人間に確認を求める

#### 具体例:
```
[Human]
Qwenを使ってコードレビューをしてくれる機能が欲しい

[AIエージェント]
承知いたしました。コードレビュー機能を実装しますね。
以下の点について確認させてください:

1. レビュー範囲: どのファイル・ディレクトリを対象にしますか？
   例: `src/` 配下のみ、`tests/` も含む、プロジェクト全体

2. 出力形式: レビュー結果はどのように表示しますか？
   例: ターミナルに直接表示、Markdownファイルに保存

3. レビュー観点: どのような項目をチェックしますか？
   例: SPEC.mdとの整合性、型安全性、テストカバレッジ

これらを教えていただいた上で、SPEC.mdに仕様を反映します。
```

### 4.2 Test-Driven Generation (TDG) 4ステップ

実装とテストを分離し、**期待値の合意** → **実装** の流れで開発を進めます。

#### Step 1: Scaffold (足場作り・テストシナリオの確認)

AIエージェントは、`SPEC.md` を元に以下の内容を人間に提示し、**確認を求める**:

- **テストすべきシナリオ一覧**（日本語）
- **期待される入力・出力**
- **エッジケースの洗い出し**

```
[AIエージェントの出力例]
# コードレビュー機能: テストシナリオ案

1. **正常系**: 指定ディレクトリ配下の全Pythonファイルをレビュー
   - 入力: `cospec review --path src/`
   - 期待: 各ファイルのレビュー結果が表示される

2. **正常系**: 単一ファイルを指定してレビュー
   - 入力: `cospec review --file src/main.py`
   - 期待: main.py のレビュー結果のみ表示

3. **異常系**: 存在しないパスを指定
   - 入力: `cospec review --path not_exists/`
   - 期待: 「指定されたパスが存在しません」エラー

4. **境界値**: 空ディレクトリを指定
   - 期待: 「レビュー対象のファイルがありません」警告

これらのテストシナリオで足りていますでしょうか？
他にテストすべきケースがあれば教えてください。
```

**重要**: 人間の承認を得るまで、テストコードの実装に**進まない**

#### Step 2: Test Generation (テストコードの生成)

承認されたシナリオを元に、**失敗するテストコード（Red）**を実装します。

- **テスト関数**: 各シナリオに対応
- **アサーション**: 期待値（日本語コメント）を記載
- **Mocks**: 外部依存（AIエージェントCLI）はモック化

#### Step 3: Implementation (実装)

テストをパスする**最小限の実装（Green）**を行います。

- **関数の雛形**: 型定義とDocstring（ドキュメントコメント）のみ
- **段階的な実装**: 最小限の実装でテストを1つずつGreenにする
- **可読性より正しさ**: Greenになることに集中し、コードの美しさは後回し

#### Step 4: Refactoring (リファクタリング)

テストがGreenな状態で、コードを整理し**可読性を高めます**。

- **抽出**: 重複処理を関数化
- **名前変更**: 意図が明確な変数名・関数名に改善
- **分割**: 巨大関数を複数の小さな関数に分割

**安全性**: テストがあるため、安心してリファクタリングできる

---

## 5. 全体ワークフローのまとめ

### 典型的な開発サイクル:

1. **要件定義フェーズ** (4.1)
   - 人間: やりたいことをざっくり伝える
   - AI: 不足情報を質問し、SPEC.mdを確定させる

2. **計画フェーズ** (3.1)
   - AI: PLAN.mdにタスクを列挙し、人間に承認を求める
   - 人間: タスク内容を確認し、Goサインを出す

3. **テスト設計フェーズ** (4.2 Step 1-2)
   - AI: テストシナリオ一覧を提示
   - 人間: シナリオを承認 or 追加指摘
   - AI: テストコード（Red）を実装し、再度確認を求める

4. **実装入力フェーズ** (4.2 Step 3-4 + 2.2)
   - AI: テストをパスする実装（Green）を実装
   - AI: `task check` を実行し、品質を保証
   - AI: リファクタリングし、可読性を向上

5. **完了・記録フェーズ** (3.2)
   - AI: PLAN.mdのタスクを完了状態に更新
   - AI: WorkingLog.mdに作業内容・判断理由を追記
   - AI: 人間に「作業完了」を報告

このフローを**標準パターン**として、全ての開発タスクに適用することで、AIエージェントと人間が効率的・効果的に協働できます。

---

## 参考資料

- [OverviewDesignThinking.md](./OverviewDesignThinking.md): 設計思想の背景
- [OverviewCodingTestingThinking.md](./OverviewCodingTestingThinking.md): コーディング・テストの考え方
- [docs/SPEC.md](../docs/SPEC.md): 機能要件
- [docs/BLUEPRINT.md](../docs/BLUEPRINT.md): 技術設計
- [docs/PLAN.md](../docs/PLAN.md): 実装計画
- [docs/WorkingLog.md](../docs/WorkingLog.md): 作業履歴